###LDA + QDA

#Preparing the data
attach(stmath)
stmath$schoolsup = as.factor(stmath$schoolsup)
stmath$sex = as.factor(stmath$sex)
stmath$guardian = as.factor(stmath$guardian)
stmath$famsup = as.factor(stmath$famsup)
stmath$paid = as.factor(stmath$paid)
stmath$activities = as.factor(stmath$activities)
stmath$nursery = as.factor(stmath$nursery)
stmath$higher = as.factor(stmath$higher)
stmath$internet = as.factor(stmath$internet)
stmath$romantic = as.factor(stmath$romantic)
stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

#LDA
#Holdout Sampling - LDA
library(MASS)

#Creating the training and test data set
library(caret) 
set.seed(06)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathlda_train=stmath[stratid,] ## Creating the Training dataset
stmathlda_test=stmath[-stratid,] ## Creating the test data set

mod_lda = lda(paid~., data = stmathlda_train)
mod_lda 

pred_lda_ho = predict(mod_lda, newdata = stmathlda_test)
head(pred_lda_ho$posterior)
head(pred_lda_ho$class)

lda_ho_conftable = table(pred_lda_ho$class, stmathlda_test$paid)
lda_ho_conftable2 = confusionMatrix(as.factor(pred_lda_ho$class),as.factor(stmathlda_test$paid), 
                                positive="1")
lda_ho_conftable2

lda_ho_acc = mean(pred_lda_ho$class==stmathlda_test$paid)
lda_ho_err = 1-lda_ho_acc
lda_ho_sens = unname(lda_ho_conftable2$table[2,2]/(colSums(lda_ho_conftable2$table))[2])
lda_ho_spec =  unname(lda_ho_conftable2$table[1,1]/(colSums(lda_ho_conftable2$table))[1])
lda_ho_kap = unname(lda_ho_conftable2$overall['Kappa'])

lda_ho_modelfitmetrics = c(lda_ho_acc,lda_ho_err,lda_ho_sens,lda_ho_spec,lda_ho_kap)
names(lda_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
lda_ho_modelfitmetrics

#CV - LDA
require(caret)
#10-fold
set.seed(07)
folds_lda_cv10 =createFolds(stmath$paid,k=10)

lda_cv10 = sapply(folds_lda_cv10, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=lda(paid~., data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

lda_cv10_modelfitmetrics = apply(lda_cv10,1,mean)
lda_cv10_modelfitmetrics

#5-fold
set.seed(08)
folds_lda_cv5 =createFolds(stmath$paid,k=5)

lda_cv5 = sapply(folds_lda_cv5, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=lda(paid~., data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

lda_cv5_modelfitmetrics = apply(lda_cv5,1,mean)
lda_cv5_modelfitmetrics

#LOOCV
set.seed(09)
folds_lda_cvlo =createFolds(stmath$paid,k=395)

lda_cvlo = sapply(folds_lda_cvlo, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=lda(paid~., data=trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

lda_cvlo_modelfitmetrics = apply(lda_cvlo,1,mean)
lda_cvlo_modelfitmetrics

comp_lda = rbind(lda_ho_modelfitmetrics,lda_cv10_modelfitmetrics, lda_cv5_modelfitmetrics, lda_cvlo_modelfitmetrics)
comp_lda

#QDA
#Holdout Sampling - QDA
#Creating the training and test data set
set.seed(10)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathqda_train=stmath[stratid,] ## Creating the Training dataset
stmathqda_test=stmath[-stratid,] ## Creating the test data set

mod_qda = qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason,data = stmathqda_train)
mod_qda 

pred_qda_ho = predict(mod_qda, newdata = stmathqda_test)
head(pred_qda_ho$posterior)
head(pred_qda_ho$class)

qda_ho_conftable = table(pred_qda_ho$class, stmathqda_test$paid)
qda_ho_conftable2 = confusionMatrix(as.factor(pred_qda_ho$class),as.factor(stmathqda_test$paid), 
                                    positive="1")
qda_ho_conftable2

qda_ho_acc = mean(pred_qda_ho$class==stmathqda_test$paid)
qda_ho_err = 1-qda_ho_acc
qda_ho_sens = unname(qda_ho_conftable2$table[2,2]/(colSums(qda_ho_conftable2$table))[2])
qda_ho_spec =  unname(qda_ho_conftable2$table[1,1]/(colSums(qda_ho_conftable2$table))[1])
qda_ho_kap = unname(qda_ho_conftable2$overall['Kappa'])

qda_ho_modelfitmetrics = c(qda_ho_acc,qda_ho_err,qda_ho_sens,qda_ho_spec,qda_ho_kap)
names(qda_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
qda_ho_modelfitmetrics

#CV - QDA
#10-fold
set.seed(11)
folds_qda_cv10 =createFolds(stmath$paid,k=10)

qda_cv10 = sapply(folds_qda_cv10, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

qda_cv10_modelfitmetrics = apply(qda_cv10,1,mean)
qda_cv10_modelfitmetrics

#5-fold
set.seed(12)
folds_qda_cv5 =createFolds(stmath$paid,k=5)

qda_cv5 = sapply(folds_qda_cv5, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

qda_cv5_modelfitmetrics = apply(qda_cv5,1,mean)
qda_cv5_modelfitmetrics

#LOOCV
set.seed(13)
folds_qda_cvlo =createFolds(stmath$paid,k=395)

qda_cvlo = sapply(folds_qda_cvlo, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason, data=trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

qda_cvlo_modelfitmetrics = apply(qda_cvlo,1,mean)
qda_cvlo_modelfitmetrics

comp_qda = rbind(qda_ho_modelfitmetrics,qda_cv10_modelfitmetrics, qda_cv5_modelfitmetrics, qda_cvlo_modelfitmetrics)
comp_qda

#Comparing LDA and QDA

comp_ldaqda = rbind(comp_lda, comp_qda)
comp_ldaqda

#SVM
#1-Normalizing our data
normalize = function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

non_num_var = stmath[,c(1,2,4,5,6,9,10,11,12,16,17,18,19,20,21,22,23)]
num_var = stmath[, c(3,7,8, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31) ]
  
Pnorm = as.data.frame(lapply(num_var, normalize))
Pnormed=cbind(Pnorm, non_num_var)
names(Pnormed)

#Creating the Testing and Training datasets using hold out sampling
set.seed(14)
stratid=createDataPartition(Pnormed$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
Normstmath_train=Pnormed[stratid,] ## Creating the Training dataset
Normstmath_test=Pnormed[-stratid,] ## Creating the test data set

#Run the SVM model
#Linear Kernel
mod_svm_lin = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="linear", cost=10, scale=FALSE)
summary(mod_svm_lin)

svm_pred_lin = predict(mod_svm_lin, newdata=Normstmath_test)
svm_lin_conftable = confusionMatrix(as.factor(svm_pred_lin),as.factor(Normstmath_test$paid), 
                positive="1")

svmlin_ho_acc = unname(svm_lin_conftable$overall[1])
svmlin_ho_err = 1-svmlin_ho_acc
svmlin_ho_sens = unname(svm_lin_conftable$table[2,2]/(colSums(svm_lin_conftable$table))[2])
svmlin_ho_spec =  unname(svm_lin_conftable$table[1,1]/(colSums(svm_lin_conftable$table))[1])
svmlin_ho_kap = unname(svm_lin_conftable$overall['Kappa'])

svmlin_ho_modelfitmetrics = c(svmlin_ho_acc,svmlin_ho_err,svmlin_ho_sens,svmlin_ho_spec,svmlin_ho_kap)
names(svmlin_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmlin_ho_modelfitmetrics

#Polynomial Kernel
mod_svm_poly = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="polynomial", cost=10, scale=FALSE)
summary(mod_svm_poly)

svm_pred_poly = predict(mod_svm_poly, newdata=Normstmath_test)
svm_pol_conftable = confusionMatrix(as.factor(svm_pred_poly),as.factor(Normstmath_test$paid), 
                positive="1")

svmpol_ho_acc = unname(svm_pol_conftable$overall[1])
svmpol_ho_err = 1-svmpol_ho_acc
svmpol_ho_sens = unname(svm_pol_conftable$table[2,2]/(colSums(svm_pol_conftable$table))[2])
svmpol_ho_spec =  unname(svm_pol_conftable$table[1,1]/(colSums(svm_pol_conftable$table))[1])
svmpol_ho_kap = unname(svm_pol_conftable$overall['Kappa'])

svmpol_ho_modelfitmetrics = c(svmpol_ho_acc,svmpol_ho_err,svmpol_ho_sens,svmpol_ho_spec,svmpol_ho_kap)
names(svmpol_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmpol_ho_modelfitmetrics

#Radial Kernel
mod_svm_rad = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="radial", cost=10, scale=FALSE)
summary(mod_svm_rad)

svm_pred_rad =predict(mod_svm_rad, newdata=Normstmath_test)
svm_rad_conftable = confusionMatrix(as.factor(svm_pred_rad),as.factor(Normstmath_test$paid), 
                positive="1")

svmrad_ho_acc = unname(svm_rad_conftable$overall[1])
svmrad_ho_err = 1-svmrad_ho_acc
svmrad_ho_sens = unname(svm_rad_conftable$table[2,2]/(colSums(svm_rad_conftable$table))[2])
svmrad_ho_spec =  unname(svm_rad_conftable$table[1,1]/(colSums(svm_rad_conftable$table))[1])
svmrad_ho_kap = unname(svm_rad_conftable$overall['Kappa'])

svmrad_ho_modelfitmetrics = c(svmrad_ho_acc,svmrad_ho_err,svmrad_ho_sens,svmrad_ho_spec,svmrad_ho_kap)
names(svmrad_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmrad_ho_modelfitmetrics

#Comparing linear, polynomial and radial kernels
comp_svm_ho = rbind(svmlin_ho_modelfitmetrics,svmpol_ho_modelfitmetrics,svmrad_ho_modelfitmetrics) 
comp_svm_ho
#Polynomial seems to be most precise

##Cross validation of the SVM with the data
#10-fold
svm_folds_cv10 = createFolds(Pnormed$paid,k=10)

svm_cv10 = sapply(svm_folds_cv10, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

svm_cv10_modelfitmetrics = apply(svm_cv10,1,mean)
svm_cv10_modelfitmetrics

#5-fold
svm_folds_cv5 = createFolds(Pnormed$paid,k=15)

svm_cv5 = sapply(svm_folds_cv5, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

svm_cv5_modelfitmetrics = apply(svm_cv5,1,mean)
svm_cv5_modelfitmetrics

#LOOCV
svm_folds_cvlo = createFolds(Pnormed$paid,k= 395)

svm_cvlo = sapply(svm_folds_cvlo, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], obj$byClass['Sensitivity'],obj$byClass['Specificity'] ,(1-obj$overall[1]) , obj$byClass[2])
  names(metrics) = c('Accuracy', 'Sensitivity','Specificity', 'Error rate', 'Kappa')
  return(metrics)
})

svm_cvlo_modelfitmetrics = apply(svm_cvlo,1,mean)
svm_cvlo_modelfitmetrics

#comparing svm across CV ks:
comp_svms = rbind(comp_svm_ho, svm_cv10_modelfitmetrics,svm_cv5_modelfitmetrics,svm_cvlo_modelfitmetrics)
comp_svms
comp_ldaqda
comp_tree_bag_fr
