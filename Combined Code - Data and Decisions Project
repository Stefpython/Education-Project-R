#### Importing data

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)

#### lm and diagnostics
#### Trees and bagging
#### Logit and Probit

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)

stmath = as.data.frame(subset(na.omit(stmath)))
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))

###Checking again
str(stmath)
dim(stmath)
attach(stmath)

###Factoring
stmath$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath$paid = as.factor(ifelse(paid=='no', 0,1))
stmath$activities = as.factor(ifelse(activities=='no', 0,1))
stmath$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath$higher = as.factor(ifelse(higher=='no', 0,1))
stmath$internet = as.factor(ifelse(internet=='no', 0,1))
stmath$romantic = as.factor(ifelse(romantic=='no', 0,1))
stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

# Creating testing and training data sets
library(MASS)
library(e1071)
library(caret) 
set.seed(14)

###Probit
###HO
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathprobit_train=stmath[stratid,] ## Creating the Training dataset
stmathprobit_test=stmath[-stratid,] ## Creating the test data set

#Using the probit model on training data
mod_probit_train=glm(paid~.,data=stmathprobit_train,family=binomial(link="probit"))
summary(mod_probit_train)
pred_probit=predict(mod_probit_train, newdata = stmathprobit_test,type = "response")

###Lets look at prediction quality
predclass_probit=rep(0,nrow(stmathprobit_test))
predclass_probit[pred_probit>0.5]=1
probit_conftable=confusionMatrix(as.factor(predclass_probit),as.factor(stmathprobit_test$paid), 
                                 positive="1")

###Testing
probit_acc = mean(predclass_probit==stmathprobit_test$paid)
probit_err = 1-probit_acc
probit_sens = unname(probit_conftable$table[2,2]/(colSums(probit_conftable$table))[2])
probit_spec =  unname(probit_conftable$table[1,1]/(colSums(probit_conftable$table))[1])
probit_kap = unname(probit_conftable$overall['Kappa'])

probit_ho_modelfitmetrics = c(probit_acc,probit_err,probit_sens,probit_spec,probit_kap)
names(probit_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
probit_ho_modelfitmetrics

############################
###Cross Validation
require(caret)

#10-fold
set.seed(13)
folds_probit_cv10 =createFolds(stmath$paid,k=10)
probit_cv10 = sapply(folds_probit_cv10, function(x)
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_CV10=rep(0,nrow(testset))
  predclass_probit_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_cv10_modelfitmetrics = apply(probit_cv10,1,mean)
probit_cv10_modelfitmetrics

############################
###5-fold
set.seed(12)
folds_probit_cv5 =createFolds(stmath$paid,k=5)
probit_cv5 = sapply(folds_probit_cv5, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_CV5=rep(0,nrow(testset))
  predclass_probit_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_cv5_modelfitmetrics = apply(probit_cv5,1,mean)
probit_cv5_modelfitmetrics

############################
###LOOCV
set.seed(11)
folds_probit_cvlo =createFolds(stmath$paid,k=394)
probit_cvlo = sapply(folds_probit_cvlo, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_CVlo=rep(0,nrow(testset))
  predclass_probit_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_cvlo_modelfitmetrics = apply(probit_cvlo,1,mean)
probit_cvlo_modelfitmetrics


###############################
###Combination
comp_probit = rbind(probit_ho_modelfitmetrics,probit_cv10_modelfitmetrics, probit_cv5_modelfitmetrics
                    , probit_cvlo_modelfitmetrics)
comp_probit

##############################################################################################
###Logit
###HO
set.seed(10)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathlogit_train=stmath[stratid,] ## Creating the Training dataset
stmathlogit_test=stmath[-stratid,] ## Creating the test data set

###Using the logit model on training data
mod_logit_train=glm(paid~.,data=stmathlogit_train,family=binomial(link="logit"))
summary(mod_logit_train)
pred_logit=predict(mod_logit_train, newdata = stmathlogit_test,type = "response")

###Lets look at prediction quality
predclass_logit=rep(0,nrow(stmathlogit_test))

predclass_logit[pred_logit>0.5]=1

logit_conftable=confusionMatrix(as.factor(predclass_logit),as.factor(stmathlogit_test$paid), 
                                positive="1")
###Testing
logit_acc = mean(predclass_logit==stmathlogit_test$paid)
logit_err = 1-logit_acc
logit_sens = unname(logit_conftable$table[2,2]/(colSums(logit_conftable$table))[2])
logit_spec =  unname(logit_conftable$table[1,1]/(colSums(logit_conftable$table))[1])
logit_kap = unname(logit_conftable$overall['Kappa'])

logit_ho_modelfitmetrics = c(logit_acc,logit_err,logit_sens,logit_spec,logit_kap)
names(logit_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')

logit_ho_modelfitmetrics
################################
###Cross Validation
###10-fold
set.seed(9)
folds_logit_cv10 =createFolds(stmath$paid,k=10)
logit_cv10 = sapply(folds_logit_cv10, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_CV10=rep(0,nrow(testset))
  predclass_logit_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_cv10_modelfitmetrics = apply(logit_cv10,1,mean)
logit_cv10_modelfitmetrics

###########################
###5-fold
set.seed(8)
folds_logit_cv5 =createFolds(stmath$paid,k=5)

logit_cv5 = sapply(folds_logit_cv5, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_CV5=rep(0,nrow(testset))
  predclass_logit_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_cv5_modelfitmetrics = apply(logit_cv5,1,mean)
logit_cv5_modelfitmetrics

##########################
###LOOCV
set.seed(7)
folds_logit_cvlo =createFolds(stmath$paid,k=394)

logit_cvlo = sapply(folds_logit_cvlo, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_CVlo=rep(0,nrow(testset))
  predclass_logit_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_cvlo_modelfitmetrics = apply(logit_cvlo,1,mean)
logit_cvlo_modelfitmetrics
###########################################################
###Combination 
comp_logit = rbind(logit_ho_modelfitmetrics,logit_cv10_modelfitmetrics, logit_cv5_modelfitmetrics
                   , logit_cvlo_modelfitmetrics)
comp_logit
################################################################################################
###Comparing Probit and Logit

comp_probit_logit = rbind(comp_probit, comp_logit)
comp_probit_logit
################################################################################################

###Replicating the above with edited dataset, removing the negative coefficient variables
###obtained by Random Forest

###Removing the negative coefficient variables from the stmath dataset; (nursery and traveltime...)
stmath_adj = as.data.frame(subset(na.omit(stmath), select = -c(nursery, freetime, absences, Fedu, 
                                                               goout,school, guardian,romantic,internet, address,traveltime)))
###Checking again
str(stmath_adj)
dim(stmath_adj)
attach(stmath_adj)

set.seed(6)

###Probit_adjusted dataset
###HO
stratid_adj=createDataPartition(stmath_adj$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmath_adj_probit_train=stmath_adj[stratid_adj,] ## Creating the Training dataset
stmath_adj_probit_test=stmath_adj[-stratid_adj,] ## Creating the test data set

#Using the probit model on training data
mod_adj_probit_train=glm(paid~.,data=stmath_adj_probit_train,family=binomial(link="probit"))
summary(mod_adj_probit_train)
pred_adj_probit=predict(mod_adj_probit_train, newdata = stmath_adj_probit_test,type = "response")

###Lets look at prediction quality
predclass_adj_probit=rep(0,nrow(stmath_adj_probit_test))
predclass_adj_probit[pred_adj_probit>0.5]=1
probit_adj_conftable=confusionMatrix(as.factor(predclass_adj_probit),as.factor(stmath_adj_probit_test$paid), 
                                     positive="1")

###Testing
probit_adj_acc = mean(predclass_adj_probit==stmath_adj_probit_test$paid)
probit_adj_err = 1-probit_adj_acc
probit_adj_sens = unname(probit_adj_conftable$table[2,2]/(colSums(probit_adj_conftable$table))[2])
probit_adj_spec =  unname(probit_adj_conftable$table[1,1]/(colSums(probit_adj_conftable$table))[1])
probit_adj_kap = unname(probit_adj_conftable$overall['Kappa'])

probit_adj_ho_modelfitmetrics = c(probit_adj_acc,probit_adj_err,probit_adj_sens,probit_adj_spec,probit_adj_kap)
names(probit_adj_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
probit_adj_ho_modelfitmetrics
############################
#10-fold
set.seed(5)
folds_probit_adj_cv10 =createFolds(stmath_adj$paid,k=10)
probit_adj_cv10 = sapply(folds_probit_adj_cv10, function(x)
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_adj_probit_CV10=rep(0,nrow(testset))
  predclass_adj_probit_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_adj_probit_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_adj_cv10_modelfitmetrics = apply(probit_adj_cv10,1,mean)
probit_adj_cv10_modelfitmetrics
############################
###5-fold
set.seed(4)
folds_probit_adj_cv5 =createFolds(stmath_adj$paid,k=5)
probit_adj_cv5 = sapply(folds_probit_adj_cv5, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_adj_CV5=rep(0,nrow(testset))
  predclass_probit_adj_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_adj_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_adj_cv5_modelfitmetrics = apply(probit_adj_cv5,1,mean)
probit_adj_cv5_modelfitmetrics
############################
###LOOCV
set.seed(3)
folds_probit_adj_cvlo =createFolds(stmath_adj$paid,k=394)
probit_adj_cvlo = sapply(folds_probit_adj_cvlo, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_adj_CVlo=rep(0,nrow(testset))
  predclass_probit_adj_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_adj_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_adj_cvlo_modelfitmetrics = apply(probit_adj_cvlo,1,mean)
probit_adj_cvlo_modelfitmetrics
###############################################################################################
###Computation combination for probit adjusted dataset
comp_probit_adj = rbind(probit_adj_ho_modelfitmetrics,probit_adj_cv10_modelfitmetrics, probit_adj_cv5_modelfitmetrics
                        , probit_adj_cvlo_modelfitmetrics)

comp_probit_adj
###Logit_adjusted dataset
###HO
set.seed(30)
stratid_logit_adj=createDataPartition(stmath_adj$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmath_logit_adj_train=stmath_adj[stratid_logit_adj,] ## Creating the Training dataset
stmath_logit_adj_test=stmath_adj[-stratid_logit_adj,] ## Creating the test data set

###Using the logit model on training data
mod_logit_adj_train=glm(paid~.,data=stmath_logit_adj_train,family=binomial(link="logit"))
summary(mod_logit_adj_train)
pred_logit_adj=predict(mod_logit_adj_train, newdata = stmath_logit_adj_test,type = "response")

###Lets look at prediction quality
predclass_logit_adj=rep(0,nrow(stmath_logit_adj_test))

predclass_logit_adj[pred_logit_adj>0.5]=1

logit_adj_conftable=confusionMatrix(as.factor(predclass_logit_adj),as.factor(stmath_logit_adj_test$paid), 
                                    positive="1")
###Testing
logit_adj_acc = mean(predclass_logit_adj==stmath_logit_adj_test$paid)
logit_adj_err = 1-logit_adj_acc
logit_adj_sens = unname(logit_adj_conftable$table[2,2]/(colSums(logit_adj_conftable$table))[2])
logit_adj_spec =  unname(logit_adj_conftable$table[1,1]/(colSums(logit_adj_conftable$table))[1])
logit_adj_kap = unname(logit_adj_conftable$overall['Kappa'])

logit_adj_ho_modelfitmetrics = c(logit_adj_acc,logit_adj_err,logit_adj_sens,logit_adj_spec,logit_adj_kap)
names(logit_adj_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')

logit_adj_ho_modelfitmetrics
################################
###Cross Validation
###10-fold
set.seed(29)
folds_logit_adj_cv10 =createFolds(stmath_adj$paid,k=10)
logit_adj_cv10 = sapply(folds_logit_adj_cv10, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_adj_CV10=rep(0,nrow(testset))
  predclass_logit_adj_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_adj_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_adj_cv10_modelfitmetrics = apply(logit_adj_cv10,1,mean)
logit_adj_cv10_modelfitmetrics

###########################
###5-fold
set.seed(28)
folds_logit_adj_cv5 =createFolds(stmath_adj$paid,k=5)

logit_adj_cv5 = sapply(folds_logit_adj_cv5, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_adj_CV5=rep(0,nrow(testset))
  predclass_logit_adj_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_adj_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_adj_cv5_modelfitmetrics = apply(logit_adj_cv5,1,mean)
logit_adj_cv5_modelfitmetrics
##########################
###LOOCV
set.seed(27)
folds_logit_adj_cvlo =createFolds(stmath_adj$paid,k=394)

logit_adj_cvlo = sapply(folds_logit_adj_cvlo, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_adj_CVlo=rep(0,nrow(testset))
  predclass_logit_adj_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor( predclass_logit_adj_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_adj_cvlo_modelfitmetrics = apply(logit_adj_cvlo,1,mean)
logit_adj_cvlo_modelfitmetrics
###############################################################################################
###Computation Combination for the logit adjusted
comp_logit_adj = rbind(logit_adj_ho_modelfitmetrics,logit_adj_cv10_modelfitmetrics, logit_adj_cv5_modelfitmetrics
                       , logit_adj_cvlo_modelfitmetrics)
comp_logit_adj

###############################################################################################
###Comparing Probit a
nd Logit adjusted dataset

comp_probit_logit_adj = rbind(comp_probit_adj, comp_logit_adj)
comp_probit_logit_adj
###############################################################################################
###Comparing Probit and Logit adjusted and normal dataset
comp_probit_logit_both = rbind(comp_probit_logit, comp_probit_logit_adj)
comp_probit_logit_both
###############################################################################################

#### LDA, QDA and SVM

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)

stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))
str(stmath)
attach(stmath)

stmath$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath$paid = as.factor(ifelse(paid=='no', 0,1))
stmath$activities = as.factor(ifelse(activities=='no', 0,1))
stmath$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath$higher = as.factor(ifelse(higher=='no', 0,1))
stmath$internet = as.factor(ifelse(internet=='no', 0,1))
stmath$romantic = as.factor(ifelse(romantic=='no', 0,1))

stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

plot(stmath$paid)

################### Tree to find out most relevant variables

names(stmath)

#Creating a sample and training dataset
library(caret)
library(tree)
set.seed(01)
stratid=createDataPartition(stmath$paid, p=0.75, list=F) ## Create the data partition
stmathtree_train=stmath[stratid,] ### Create the training data set
stmathtree_test=stmath[-stratid,] ## Create the testing data set

set.seed(02)
tree.stmath=tree(paid~., 
                 data=stmathtree_train) ## Estimates the classification tree

summary(tree.stmath)
#The misclassification reported is the entropy and it is around 20%
plot(tree.stmath)
text(tree.stmath, pretty=0)

#Cross-validating the tree
cvtree.stmath=cv.tree(tree.stmath, K=10)
summary(cvtree.stmath)

#Finding and implementing optimal tree size
plot(cvtree.stmath$size, cvtree.stmath$dev, type='b')
prune.stmath=prune.tree(tree.stmath, best=5) ## the best parameter limits the size of the tree
plot(prune.stmath) 
text(prune.stmath, pretty=0)
#Most important variables: famsup + reason + famsize + G3 + famsup:G3 + famsup:reason + famsup:reason:famsize

#Test Error
tree.stmath.pred=predict(prune.stmath, stmathtree_test, 
                         type="class") 
library(e1071)
conftabletree = confusionMatrix(as.factor(tree.stmath.pred),as.factor(stmathtree_test$paid), 
                                positive="1")
conftabletree

accuracy_tree = mean(tree.stmath.pred == stmathtree_test$paid)
errorrate_tree = 1-accuracy_tree
sensitivity_tree = unname(conftabletree$table[2,2]/(colSums(conftabletree$table))[2])
specificity_tree =  unname(conftabletree$table[1,1]/(colSums(conftabletree$table))[1])
kappa_tree = unname(conftabletree$overall['Kappa'])

modelfitmetrics_stmathtree = c(accuracy_tree,errorrate_tree,sensitivity_tree,specificity_tree,kappa_tree)
names(modelfitmetrics_stmathtree) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
modelfitmetrics_stmathtree

###Bagging
#install.packages("randomForest", dependencies=TRUE)
library(randomForest)
set.seed(03)
bag.stmath = randomForest(paid~., data=stmathtree_train, 
                          mtry=(ncol(stmathtree_train)-1),importance=T,
                          ntree=500)
bag.stmath
bag.stmath.pred = predict(bag.stmath, stmathtree_test, type = 'class')
conftabletree = confusionMatrix(as.factor(bag.stmath.pred),as.factor(stmathtree_test$paid), 
                                positive="1")

accuracy_bag = mean(bag.stmath.pred == stmathtree_test$paid)
errorrate_bag = 1-accuracy_bag
sensitivity_bag = unname(bag.stmath$confusion[2,2]/(colSums(bag.stmath$confusion))[2])
specificity_bag =  unname(bag.stmath$confusion[1,1]/(colSums(bag.stmath$confusion))[1])
kappa_bag = unname(conftabletree$overall['Kappa'])

modelfitmetrics_stmathbag = c(accuracy_bag,errorrate_bag,sensitivity_bag,specificity_bag,kappa_bag)
names(modelfitmetrics_stmathbag) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
modelfitmetrics_stmathbag

barplot(sort(bag.stmath$importance[,1]), 
        main="Variable importance from Bagging")
sort(bag.stmath$importance[,1])
#Most important variables: famsup + G3 + reason + Medu + Walc + Dalc + Mjob + health + studytime + sex + absences + age + famsize + goout + Pstatus + romantic + schoolsup + higher + school + address + Fedu + traveltime

#RandomForest 
set.seed(04)
rf.stmath = randomForest(paid~., data=stmathtree_train, 
                          mtry=sqrt((ncol(stmathtree_train)-1)),importance=T,
                          ntree=500)
rf.stmath
rf.stmath.pred = predict(bag.stmath, stmathtree_test, type = 'class')
conftabletree = confusionMatrix(as.factor(rf.stmath.pred),as.factor(stmathtree_test$paid), 
                                positive="1")

accuracy_rf = mean(rf.stmath.pred == stmathtree_test$paid)
errorrate_rf = 1-accuracy_rf
sensitivity_rf = unname(rf.stmath$confusion[2,2]/(colSums(rf.stmath$confusion))[2])
specificity_rf =  unname(rf.stmath$confusion[1,1]/(colSums(rf.stmath$confusion))[1])
kappa_rf = unname(conftabletree$overall['Kappa'])

modelfitmetrics_stmathrf = c(accuracy_rf,errorrate_rf,sensitivity_rf,specificity_rf,kappa_rf)
names(modelfitmetrics_stmathrf) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
modelfitmetrics_stmathrf

barplot(sort(rf.stmath$importance[,1]), 
        main="Variable importance from Bagging")
sort(rf.stmath$importance[,1])
#Most important variables: famsup + G3 + reason + Medu + Walc + Dalc + Mjob + health + studytime + sex + absences + age + famsize + goout + Pstatus + romantic + schoolsup + higher + school + address + Fedu + traveltime

#Comparing Tree, Bagging and Random Forest
comp_tree_bag_fr = rbind(modelfitmetrics_stmathtree,modelfitmetrics_stmathbag, modelfitmetrics_stmathrf)
comp_tree_bag_fr


############ For the lines of code below, I tried doing LMs, I am not sure if lines 111 to 142 are necessary to re-clean the data to run lms, 
#I stopped here before trying in depth.

stmath$schoolsup = as.numeric(stmath$schoolsup)
stmath$sex = as.numeric(stmath$sex)
stmath$guardian = as.numeric(stmath$guardian)
stmath$famsup = as.numeric(stmath$famsup)
stmath$paid = as.numeric(stmath$paid)
stmath$activities = as.numeric(stmath$activities)
stmath$nursery = as.numeric(stmath$nursery)
stmath$higher = as.numeric(stmath$higher)
stmath$internet = as.numeric(stmath$internet)
stmath$romantic = as.numeric(stmath$romantic)

attach(stmath)
stmath$schoolsup = ifelse(schoolsup==1, 0,1)
stmath$sex = ifelse(sex== 1, 0,1)
stmath$guardian = ifelse(guardian == 1, 0,1)
stmath$famsup = ifelse(famsup==1, 0,1)
stmath$paid = ifelse(paid==1, 0,1)
stmath$activities = ifelse(activities==1, 0,1)
stmath$nursery = ifelse(nursery==1, 0,1)
stmath$higher = ifelse(higher==1, 0,1)
stmath$internet = ifelse(internet==1, 0,1)
stmath$romantic = ifelse(romantic==1, 0,1)

#Deriving linear models from tree and bag. Tree should perform better
mod1_lm = lm(paid~famsup+reason+famsize + G3 + famsup:G3 + famsup:reason + famsup:reason:famsize, data = stmath)
summary(mod1_lm)
library(car)
vif(mod1_lm)

mod2_lm = lm(paid~., data = stmath)
summary(mod2_lm)
vif(mod2_lm)

regmetrics=function(model,data, y)
{
  p=predict(model,newdata=data)
  MAPE=sum((abs(y-p))/y)/nrow(data)
  MAD=sum((y-p)^2)/nrow(data)
  MSE=sum(abs(y-p))/nrow(data)
  x = cbind(MAPE,MAD,MSE)
  return (x)
}

set.seed(05)
mod1_folds=createFolds(stmath$paid,k=10)

regmetkfold_mod1_lm = sapply(mod1_folds, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=lm(paid~famsup+reason+famsize + G3 + famsup:G3 + famsup:reason + famsup:reason:famsize, data = trainset)
  msp=regmetrics(model, testset, testset$schoolsup)
  names(msp) = c('MAPE', 'MAD', 'MSE')
  return(msp)
})

regmetkfold_mod1_lm
regmetkfold_mod1_lm = apply(regmetkfold_mod1_lm,1,mean)
regmetkfold_mod1_lm

###LDA + QDA

#Preparing the data
attach(stmath)
stmath$schoolsup = as.factor(stmath$schoolsup)
stmath$sex = as.factor(stmath$sex)
stmath$guardian = as.factor(stmath$guardian)
stmath$famsup = as.factor(stmath$famsup)
stmath$paid = as.factor(stmath$paid)
stmath$activities = as.factor(stmath$activities)
stmath$nursery = as.factor(stmath$nursery)
stmath$higher = as.factor(stmath$higher)
stmath$internet = as.factor(stmath$internet)
stmath$romantic = as.factor(stmath$romantic)
stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

#LDA
#Holdout Sampling - LDA
library(MASS)

#Creating the training and test data set
library(caret) 
set.seed(06)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathlda_train=stmath[stratid,] ## Creating the Training dataset
stmathlda_test=stmath[-stratid,] ## Creating the test data set

mod_lda = lda(paid~., data = stmathlda_train)
mod_lda 

pred_lda_ho = predict(mod_lda, newdata = stmathlda_test)
head(pred_lda_ho$posterior)
head(pred_lda_ho$class)

lda_ho_conftable = table(pred_lda_ho$class, stmathlda_test$paid)
lda_ho_conftable2 = confusionMatrix(as.factor(pred_lda_ho$class),as.factor(stmathlda_test$paid), 
                                positive="1")
lda_ho_conftable2

lda_ho_acc = mean(pred_lda_ho$class==stmathlda_test$paid)
lda_ho_err = 1-lda_ho_acc
lda_ho_sens = unname(lda_ho_conftable2$table[2,2]/(colSums(lda_ho_conftable2$table))[2])
lda_ho_spec =  unname(lda_ho_conftable2$table[1,1]/(colSums(lda_ho_conftable2$table))[1])
lda_ho_kap = unname(lda_ho_conftable2$overall['Kappa'])

lda_ho_modelfitmetrics = c(lda_ho_acc,lda_ho_err,lda_ho_sens,lda_ho_spec,lda_ho_kap)
names(lda_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
lda_ho_modelfitmetrics

#CV - LDA
require(caret)
#10-fold
set.seed(07)
folds_lda_cv10 =createFolds(stmath$paid,k=10)

lda_cv10 = sapply(folds_lda_cv10, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=lda(paid~., data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1],(1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

lda_cv10_modelfitmetrics = apply(lda_cv10,1,mean)
lda_cv10_modelfitmetrics

#5-fold
set.seed(08)
folds_lda_cv5 =createFolds(stmath$paid,k=5)

lda_cv5 = sapply(folds_lda_cv5, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=lda(paid~., data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

lda_cv5_modelfitmetrics = apply(lda_cv5,1,mean)
lda_cv5_modelfitmetrics

#LOOCV
set.seed(09)
folds_lda_cvlo =createFolds(stmath$paid,k=395)

lda_cvlo = sapply(folds_lda_cvlo, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=lda(paid~., data=trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

lda_cvlo_modelfitmetrics = apply(lda_cvlo,1,mean)
lda_cvlo_modelfitmetrics

comp_lda = rbind(lda_ho_modelfitmetrics,lda_cv10_modelfitmetrics, lda_cv5_modelfitmetrics, lda_cvlo_modelfitmetrics)
comp_lda

#QDA
#Holdout Sampling - QDA
#Creating the training and test data set
set.seed(10)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathqda_train=stmath[stratid,] ## Creating the Training dataset
stmathqda_test=stmath[-stratid,] ## Creating the test data set

mod_qda = qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason,data = stmathqda_train)
mod_qda 

pred_qda_ho = predict(mod_qda, newdata = stmathqda_test)
head(pred_qda_ho$posterior)
head(pred_qda_ho$class)

qda_ho_conftable = table(pred_qda_ho$class, stmathqda_test$paid)
qda_ho_conftable2 = confusionMatrix(as.factor(pred_qda_ho$class),as.factor(stmathqda_test$paid), 
                                    positive="1")
qda_ho_conftable2

qda_ho_acc = mean(pred_qda_ho$class==stmathqda_test$paid)
qda_ho_err = 1-qda_ho_acc
qda_ho_sens = unname(qda_ho_conftable2$table[2,2]/(colSums(qda_ho_conftable2$table))[2])
qda_ho_spec =  unname(qda_ho_conftable2$table[1,1]/(colSums(qda_ho_conftable2$table))[1])
qda_ho_kap = unname(qda_ho_conftable2$overall['Kappa'])

qda_ho_modelfitmetrics = c(qda_ho_acc,qda_ho_err,qda_ho_sens,qda_ho_spec,qda_ho_kap)
names(qda_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
qda_ho_modelfitmetrics

#CV - QDA
#10-fold
set.seed(11)
folds_qda_cv10 =createFolds(stmath$paid,k=10)

qda_cv10 = sapply(folds_qda_cv10, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

qda_cv10_modelfitmetrics = apply(qda_cv10,1,mean)
qda_cv10_modelfitmetrics

#5-fold
set.seed(12)
folds_qda_cv5 =createFolds(stmath$paid,k=5)

qda_cv5 = sapply(folds_qda_cv5, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

qda_cv5_modelfitmetrics = apply(qda_cv5,1,mean)
qda_cv5_modelfitmetrics

#LOOCV
set.seed(13)
folds_qda_cvlo =createFolds(stmath$paid,k=395)

qda_cvlo = sapply(folds_qda_cvlo, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=qda(paid~famsup+famsize+G3+reason+famsup:G3+famsup:reason, data=trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

qda_cvlo_modelfitmetrics = apply(qda_cvlo,1,mean)
qda_cvlo_modelfitmetrics

comp_qda = rbind(qda_ho_modelfitmetrics,qda_cv10_modelfitmetrics, qda_cv5_modelfitmetrics, qda_cvlo_modelfitmetrics)
comp_qda

#Comparing LDA and QDA

comp_ldaqda = rbind(comp_lda, comp_qda)
comp_ldaqda

#SVM
#1-Normalizing our data
normalize = function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

non_num_var = stmath[,c(1,2,4,5,6,9,10,11,12,16,17,18,19,20,21,22,23)]
num_var = stmath[, c(3,7,8, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31) ]
  
Pnorm = as.data.frame(lapply(num_var, normalize))
Pnormed=cbind(Pnorm, non_num_var)
names(Pnormed)

#Creating the Testing and Training datasets using hold out sampling
set.seed(14)
stratid=createDataPartition(Pnormed$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
Normstmath_train=Pnormed[stratid,] ## Creating the Training dataset
Normstmath_test=Pnormed[-stratid,] ## Creating the test data set

#Run the SVM model
#Linear Kernel
mod_svm_lin = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="linear", cost=10, scale=FALSE)
summary(mod_svm_lin)

svm_pred_lin = predict(mod_svm_lin, newdata=Normstmath_test)
svm_lin_conftable = confusionMatrix(as.factor(svm_pred_lin),as.factor(Normstmath_test$paid), 
                positive="1")

svmlin_ho_acc = unname(svm_lin_conftable$overall[1])
svmlin_ho_err = 1-svmlin_ho_acc
svmlin_ho_sens = unname(svm_lin_conftable$table[2,2]/(colSums(svm_lin_conftable$table))[2])
svmlin_ho_spec =  unname(svm_lin_conftable$table[1,1]/(colSums(svm_lin_conftable$table))[1])
svmlin_ho_kap = unname(svm_lin_conftable$overall['Kappa'])

svmlin_ho_modelfitmetrics = c(svmlin_ho_acc,svmlin_ho_err,svmlin_ho_sens,svmlin_ho_spec,svmlin_ho_kap)
names(svmlin_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmlin_ho_modelfitmetrics

#Polynomial Kernel
mod_svm_poly = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="polynomial", cost=10, scale=FALSE)
summary(mod_svm_poly)

svm_pred_poly = predict(mod_svm_poly, newdata=Normstmath_test)
svm_pol_conftable = confusionMatrix(as.factor(svm_pred_poly),as.factor(Normstmath_test$paid), 
                positive="1")

svmpol_ho_acc = unname(svm_pol_conftable$overall[1])
svmpol_ho_err = 1-svmpol_ho_acc
svmpol_ho_sens = unname(svm_pol_conftable$table[2,2]/(colSums(svm_pol_conftable$table))[2])
svmpol_ho_spec =  unname(svm_pol_conftable$table[1,1]/(colSums(svm_pol_conftable$table))[1])
svmpol_ho_kap = unname(svm_pol_conftable$overall['Kappa'])

svmpol_ho_modelfitmetrics = c(svmpol_ho_acc,svmpol_ho_err,svmpol_ho_sens,svmpol_ho_spec,svmpol_ho_kap)
names(svmpol_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmpol_ho_modelfitmetrics

#Radial Kernel
mod_svm_rad = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="radial", cost=10, scale=FALSE)
summary(mod_svm_rad)

svm_pred_rad =predict(mod_svm_rad, newdata=Normstmath_test)
svm_rad_conftable = confusionMatrix(as.factor(svm_pred_rad),as.factor(Normstmath_test$paid), 
                positive="1")

svmrad_ho_acc = unname(svm_rad_conftable$overall[1])
svmrad_ho_err = 1-svmrad_ho_acc
svmrad_ho_sens = unname(svm_rad_conftable$table[2,2]/(colSums(svm_rad_conftable$table))[2])
svmrad_ho_spec =  unname(svm_rad_conftable$table[1,1]/(colSums(svm_rad_conftable$table))[1])
svmrad_ho_kap = unname(svm_rad_conftable$overall['Kappa'])

svmrad_ho_modelfitmetrics = c(svmrad_ho_acc,svmrad_ho_err,svmrad_ho_sens,svmrad_ho_spec,svmrad_ho_kap)
names(svmrad_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmrad_ho_modelfitmetrics

#Comparing linear, polynomial and radial kernels
comp_svm_ho = rbind(svmlin_ho_modelfitmetrics,svmpol_ho_modelfitmetrics,svmrad_ho_modelfitmetrics) 
comp_svm_ho
#Polynomial seems to be most precise

##Cross validation of the SVM with the data
#10-fold
svm_folds_cv10 = createFolds(Pnormed$paid,k=10)

svm_cv10 = sapply(svm_folds_cv10, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

svm_cv10_modelfitmetrics = apply(svm_cv10,1,mean)
svm_cv10_modelfitmetrics

#5-fold
svm_folds_cv5 = createFolds(Pnormed$paid,k=15)

svm_cv5 = sapply(svm_folds_cv5, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

svm_cv5_modelfitmetrics = apply(svm_cv5,1,mean)
svm_cv5_modelfitmetrics

#LOOCV
svm_folds_cvlo = createFolds(Pnormed$paid,k= 395)

svm_cvlo = sapply(svm_folds_cvlo, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

svm_cvlo_modelfitmetrics = apply(svm_cvlo,1,mean)
svm_cvlo_modelfitmetrics

#comparing svm across CV ks:
comp_svms = rbind(comp_svm_ho, svm_cv10_modelfitmetrics,svm_cv5_modelfitmetrics,svm_cvlo_modelfitmetrics)
comp_svms
comp_ldaqda
comp_tree_bag_fr

#### kNN and Naive Bayes

#0 - Data Manipulation

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)

stmath_kNN_NB = stmath
stmath_kNN_NB = as.data.frame(subset(na.omit(stmath_kNN_NB)))
stmath_kNN_NB = stmath_kNN_NB[, -c(9, 10, 11, 31, 32)]
str(stmath_kNN_NB)
attach(stmath_kNN_NB)

stmath_kNN_NB$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath_kNN_NB$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath_kNN_NB$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath_kNN_NB$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath_kNN_NB$paid = as.factor(ifelse(paid=='no', 0,1))
stmath_kNN_NB$activities = as.factor(ifelse(activities=='no', 0,1))
stmath_kNN_NB$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath_kNN_NB$higher = as.factor(ifelse(higher=='no', 0,1))
stmath_kNN_NB$internet = as.factor(ifelse(internet=='no', 0,1))
stmath_kNN_NB$romantic = as.factor(ifelse(romantic=='no', 0,1))

stmath_kNN_NB$school = as.factor(ifelse(school=='GP', 0,1))
stmath_kNN_NB$address = as.factor(ifelse(address=='R', 0,1))
stmath_kNN_NB$famsize = as.factor(ifelse(famsize=='LE3', 0,1))
stmath_kNN_NB$Pstatus = as.factor(ifelse(Pstatus=='A', 0,1))

## kNN

## Normalizing our data
normalize_kNN_NB = function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

non_num_var_kNN_NB = stmath_kNN_NB[, c(1,2,4,5,6,9,13,14,15,16,17,18,19,20)]
num_var_kNN_NB = stmath_kNN_NB[, c(3,7,8,10,11,12,21,22,23,24,25,26,27,28)]

Pnorm_kNN_NB = as.data.frame(lapply(num_var_kNN_NB, normalize_kNN_NB))
Pnormed_kNN_NB=cbind(Pnorm_kNN_NB, non_num_var_kNN_NB)
names(Pnormed_kNN_NB)

# Creating testing and training data sets

library(caret) 
set.seed(14)
stratid_kNN_NB=createDataPartition(Pnormed_kNN_NB$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
Normstmath_kNN_NB_train=Pnormed_kNN_NB[stratid_kNN_NB,] ## Creating the Training dataset
Normstmath_kNN_NB_test=Pnormed_kNN_NB[-stratid_kNN_NB,] ## Creating the test data set

## Run the KNN algorithm on the Training data set

library(class)

knn_mod = knn(train=Normstmath_kNN_NB_train[,-23], test=Normstmath_kNN_NB_test[,-23], cl=Normstmath_kNN_NB_train[,23], k=16)

## Test model predictions using the confusion matrix

library(e1071)
confusionMatrix(as.factor(knn_mod),as.factor(Normstmath_kNN_NB_test[,23]), 
                positive="1")

## KNN with cross validation
folds_kNN_NB=createFolds(Pnormed_kNN_NB$paid,k=10)

knn_cvaccuracy = sapply(folds_kNN_NB, function(x){
  trainset=Pnormed_kNN_NB[-x,]
  testset=Pnormed_kNN_NB[x,]
  model=knn(trainset[,-23], test=testset[,-23], cl=trainset[,23], k=16)
  obj=confusionMatrix(as.factor(model),as.factor(testset[,23]), 
                      positive="1")
  metrics_kNN_NB=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics_kNN_NB)
})

knn_cvaccuracy

### Naive Bayes Classifier

library(e1071)

## Holdout sampling apporach

Normstmath_kNN_NB_train_nb=Normstmath_kNN_NB_train[,-23] ## Pulling the features out from the train set
Normstmath_kNN_NB_test_nb=Normstmath_kNN_NB_test[,-23] ## Pulling the features set out of the test set

nbmath=naiveBayes(Normstmath_kNN_NB_train_nb, as.factor(Normstmath_kNN_NB_train[,23])) ## Run the Naive Bayes model

nbpred=predict(nbmath, Normstmath_kNN_NB_test_nb, type="class") ## Generate Test Predictions


## Generate the confusion matrix

confusionMatrix(as.factor(nbpred),as.factor(Normstmath_kNN_NB_test[,23]), 
                positive="1")

## Naive Bayes with cross validation
nb_folds=createFolds(Pnormed_kNN_NB$paid,k=10)

nb_cvaccuracy = sapply(folds_kNN_NB, function(x){
  trainset=Pnormed_kNN_NB[-x,]
  testset=Pnormed_kNN_NB[x,]
  
  trainx=trainset[,-23]
  testx=testset[,-23]
  
  model=naiveBayes(trainx, as.factor(trainset[,23]))
  npred=predict(model, testx, type="class")
  
  obj=confusionMatrix(as.factor(npred),as.factor(testset[,23]), 
                      positive="1")
  metrics=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics)
})

nb_cvaccuracy

## Running kNN and Naive Bayes with variables removed

remove(stmath_kNN_NB)

stmath_kNN_NB = stmath

dim(stmath_kNN_NB)
names(stmath_kNN_NB)

#0 - Data Manipulation
stmath_kNN_NB = as.data.frame(subset(na.omit(stmath_kNN_NB)))
stmath_kNN_NB = stmath_kNN_NB[, -c(1, 8, 9, 10, 11, 12, 13, 20, 22, 23, 25, 26, 30, 31, 32)]
str(stmath_kNN_NB)
attach(stmath_kNN_NB)

stmath_kNN_NB$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath_kNN_NB$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath_kNN_NB$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath_kNN_NB$paid = as.factor(ifelse(paid=='no', 0,1))
stmath_kNN_NB$activities = as.factor(ifelse(activities=='no', 0,1))
stmath_kNN_NB$higher = as.factor(ifelse(higher=='no', 0,1))

stmath_kNN_NB$address = as.factor(ifelse(address=='R', 0,1))
stmath_kNN_NB$famsize = as.factor(ifelse(famsize=='LE3', 0,1))
stmath_kNN_NB$Pstatus = as.factor(ifelse(Pstatus=='A', 0,1))

## kNN

LESSnon_num_var = stmath_kNN_NB[, c(1,3,4,5,9,10,11,12,13)]
LESSnum_var = stmath_kNN_NB[, c(2,6,7,8,14,15,16,17,18)]

LESSPnorm = as.data.frame(lapply(LESSnum_var, normalize_kNN_NB))
LESSPnormed=cbind(LESSPnorm, LESSnon_num_var)
names(LESSPnormed)

# Creating testing and training data sets

library(caret) 
set.seed(14)
LESSstratid=createDataPartition(LESSPnormed$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
LESSNormstmath_train=LESSPnormed[LESSstratid,] ## Creating the Training dataset
LESSNormstmath_test=LESSPnormed[-LESSstratid,] ## Creating the test data set

## Run the KNN algorithm on the Training data set

library(class)

LESSknn_mod = knn(train=LESSNormstmath_train[,-16], test=LESSNormstmath_test[,-16], cl=LESSNormstmath_train[,16], k=16)

## Test model predictions using the confusion matrix

library(e1071)
confusionMatrix(as.factor(LESSknn_mod),as.factor(LESSNormstmath_test[,16]), 
                positive="1")

## KNN with cross validation
LESSfolds=createFolds(LESSPnormed$paid,k=10)

LESSknn_cvaccuracy = sapply(LESSfolds, function(x){
  trainset=LESSPnormed[-x,]
  testset=LESSPnormed[x,]
  model=knn(trainset[,-16], test=testset[,-16], cl=trainset[,16], k=16)
  obj=confusionMatrix(as.factor(model),as.factor(testset[,16]), 
                      positive="1")
  metrics=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics)
})

LESSknn_cvaccuracy

### Naive Bayes Classifier

library(e1071)

## Holdout sampling apporach

LESSNormstmath_train_nb=LESSNormstmath_train[,-16] ## Pulling the features out from the train set
LESSNormstmath_test_nb=LESSNormstmath_test[,-16] ## Pulling the features set out of the test set

LESSnbmath=naiveBayes(LESSNormstmath_train_nb, as.factor(LESSNormstmath_train[,16])) ## Run the Naive Bayes model

LESSnbpred=predict(LESSnbmath, LESSNormstmath_test_nb, type="class") ## Generate Test Predictions


## Generate the confusion matrix

confusionMatrix(as.factor(LESSnbpred),as.factor(LESSNormstmath_test[,16]), 
                positive="1")

## Naive Bayes with cross validation
LESSnb_folds=createFolds(LESSPnormed$paid,k=10)

LESSnb_cvaccuracy = sapply(LESSfolds, function(x){
  trainset=LESSPnormed[-x,]
  testset=LESSPnormed[x,]
  
  LESStrainx=trainset[,-16]
  LESStestx=testset[,-16]
  
  model=naiveBayes(LESStrainx, as.factor(trainset[,16]))
  npred=predict(model, LESStestx, type="class")
  
  obj=confusionMatrix(as.factor(npred),as.factor(testset[,16]), 
                      positive="1")
  metrics=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics)
})

LESSnb_cvaccuracy

## End of kNN and Naive Bayes section.

#### SVM
#### Additional analysis for conclusion
