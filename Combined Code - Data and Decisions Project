#### Importing data

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')
stmath = as.data.frame(subset(na.omit(stmath)))
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))
dim(stmath)
names(stmath)

#### lm and diagnostics

stmath_numeric= student.mat
stmath_numeric= as.data.frame(subset(na.omit(stmath_numeric), select = -c(G1, G2)))
stmath_numeric$schoolsup = ifelse(schoolsup=='no', 0,1)
stmath_numeric$sex = ifelse(sex== 'M', 0,1)
stmath_numeric$guardian = ifelse(guardian == 'father', 0,1)
stmath_numeric$famsup = ifelse(famsup=='no', 0,1)
stmath_numeric$paid = ifelse(paid=='no', 0,1)
stmath_numeric$activities = ifelse(activities=='no', 0,1)
stmath_numeric$nursery = ifelse(nursery=='no', 0,1)
stmath_numeric$higher = ifelse(higher=='no', 0,1)
stmath_numeric$internet = ifelse(internet=='no', 0,1)
stmath_numeric$romantic = ifelse(romantic=='no', 0,1)

mod_lm=lm(paid~.,data=stmath_numeric)
summary(mod_lm)

step(lm(paid~.,data=stmath_numeric))

mod_lm_step =lm(formula = paid ~ sex + reason + guardian + studytime + failures + 
                  famsup + nursery + higher + internet + Walc, data = stmath_numeric)
summary(mod_lm_step)
p_lm=predict(mod_lm_step)
summary(p_lm)
plot(lm(paid ~ sex + reason + guardian + studytime + failures + 
          famsup + nursery + higher + internet + Walc, data = stmath_numeric))
library(tseries)
jarque.bera.test(mod0$residuals)
plot(mod_lm_step$residuals)

library(lmtest)
bptest(mod_lm_step)

#### Trees and bagging

stmath=student.mat
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))
str(stmath)
attach(stmath)
dim(stmath)


##factor
stmath$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath$paid = as.factor(ifelse(paid=='no', 0,1))
stmath$activities = as.factor(ifelse(activities=='no', 0,1))
stmath$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath$higher = as.factor(ifelse(higher=='no', 0,1))
stmath$internet = as.factor(ifelse(internet=='no', 0,1))
stmath$romantic = as.factor(ifelse(romantic=='no', 0,1))

stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

####



stmath$paid = as.factor(stmath$paid)
class(stmath$paid)
summary(stmath$paid)

set.seed(11) ## Ensure that results are reproducible

library(caret) 
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmath_strat_train=stmath[stratid,] ## Creating the Training dataset
stmath_strat_test=stmath[-stratid,] ## Creating the test data set

### Fitting a simple regression tree

library(tree)
summary(stmath_strat_train$paid)
set.seed(66)
tree.stmath=tree(paid~., data=stmath_strat_train) ## Fit the tree on the training dataset
summary(tree.stmath) 
## Note that only few variables have been used in constructing
## the dataset

## Lets plot the tree
plot(tree.stmath)
text(tree.stmath, pretty=0)

## Evaluate the test error in this case using tree
tree.pred=predict(tree.stmath,stmath_strat_test,type="class")
summary(tree.pred)
summary(stmath_strat_test$paid)
summary(stmath_strat_train$paid)
prop.table(table(tree.pred, stmath_strat_test$paid))
mean(tree.pred==stmath_strat_test$paid) 
library(e1071)
conftabletree1 = confusionMatrix(as.factor(tree.pred),as.factor(stmath_strat_test$paid), 
                                positive="1")
conftabletree1
## Cross validating the tree
cvtree.stmath=cv.tree(tree.stmath, K=10)

## the important parameters to consider is how increasing the size of the tree
## reduces the deviance in the fit

plot(cvtree.stmath$size, cvtree.stmath$dev, type='b')
prune.stmath=prune.tree(tree.stmath, best=5) ## the best parameter limits the size of the tree
plot(prune.stmath) 
text(prune.stmath, pretty=0)

tree.stmath.pred=predict(prune.stmath, stmath_strat_test, 
                         type="class")
summary(tree.stmath.pred)
library(e1071)
conftabletree = confusionMatrix(as.factor(tree.stmath.pred),as.factor(stmath_strat_test$paid), 
                                positive="1")
conftabletree



## Next lets prune the tree and cross validate using cost complexity to see if we can
## improve fit

set.seed(11)

cv.stmath.tree=cv.tree(tree.stmath) ## cv tree

## size is the size of the tree, dev is the deviance correction, k is the complexity parameter
## The default pruning occurs using deviance as the criteria
## we can change that to misclassification rate by using an additional parameter called FUN

cv.stmath.tree.mis=cv.tree(tree.stmath, FUN=prune.misclass)

## LEts now plot the error rate as a function of size and cost

par(mfrow=c(1,2))

plot(cv.stmath.tree.mis$size, cv.stmath.tree.mis$dev, type="b")
plot(cv.stmath.tree.mis$k, cv.stmath.tree.mis$dev, type="b")

## Cross validation results suggest that maybe the right size is around 7-8 nodes
## lets prune the tree to 6 nodes and see if we can improve the results

prune.stmath2=prune.misclass(tree.stmath, best=2)
plot(prune.stmath2) 
text(prune.stmath2, pretty=0)
## Lets check our out of sample predictions

prune.pred2=predict(prune.stmath2, stmath_strat_test, type="class")
table(prune.pred2, stmath_strat_test$paid)
mean(prune.pred2==stmath_strat_test$paid)


conftabletree2 = confusionMatrix(as.factor(prune.pred2),as.factor(stmath_strat_test$paid), 
                                positive="1")
conftabletree2
## not really much of an improvement with the tree

### We could next try bagging

library(randomForest)

set.seed(11)
bag.stmath=randomForest(stmath_strat_train$paid~.,
                    data=stmath_strat_train, 
                    mtry=(ncol(stmath_strat_train)-1),
                    importance=T)
bag.stmath

## Lets get the prediction error now
set.seed(66)
pred.bag.stmath=predict(bag.stmath, stmath_strat_test, type="class")
mean(pred.bag.stmath==stmath_strat_test$paid)


conftablebag = confusionMatrix(as.factor(pred.bag.stmath),as.factor(stmath_strat_test$paid), 
                                 positive="1")
conftablebag

par(mfrow=c(1,1))
barplot(sort(bag.stmath$importance[,1]))
sort(bag.stmath$importance[,1])

### Try the same for the random forest
set.seed(11)
bag.stmath.sqrt=randomForest(stmath_strat_train$paid~.,
                    data=stmath_strat_train, 
                    mtry=sqrt(ncol(stmath_strat_train)-1), 
                    importance=T,
                    ntrees=5000)
bag.stmath.sqrt

## Lets get the prediction error now
pred.bag.stmath.sqrt=predict(bag.stmath.sqrt, stmath_strat_test, type="class")
mean(pred.bag.stmath.sqrt==stmath_strat_test$paid)

conftablebag.sqrt = confusionMatrix(as.factor(pred.bag.stmath.sqrt),as.factor(stmath_strat_test$paid), 
                                positive="1")
conftablebag.sqrt

par(mfrow=c(1,1))
barplot(sort(bag.stmath.sqrt$importance[,1]))
sort(bag.stmath.sqrt$importance[,1])


#### Logit and Probit

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)

stmath = as.data.frame(subset(na.omit(stmath)))
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))

###Checking again
str(stmath)
dim(stmath)
attach(stmath)

###Factoring
stmath$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath$paid = as.factor(ifelse(paid=='no', 0,1))
stmath$activities = as.factor(ifelse(activities=='no', 0,1))
stmath$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath$higher = as.factor(ifelse(higher=='no', 0,1))
stmath$internet = as.factor(ifelse(internet=='no', 0,1))
stmath$romantic = as.factor(ifelse(romantic=='no', 0,1))
stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

# Creating testing and training data sets
library(MASS)
library(e1071)
library(caret) 
set.seed(14)

###Probit
###HO
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathprobit_train=stmath[stratid,] ## Creating the Training dataset
stmathprobit_test=stmath[-stratid,] ## Creating the test data set

#Using the probit model on training data
mod_probit_train=glm(paid~.,data=stmathprobit_train,family=binomial(link="probit"))
summary(mod_probit_train)
pred_probit=predict(mod_probit_train, newdata = stmathprobit_test,type = "response")

###Lets look at prediction quality
predclass_probit=rep(0,nrow(stmathprobit_test))
predclass_probit[pred_probit>0.5]=1
probit_conftable=confusionMatrix(as.factor(predclass_probit),as.factor(stmathprobit_test$paid), 
                                 positive="1")

###Testing
probit_acc = mean(predclass_probit==stmathprobit_test$paid)
probit_err = 1-probit_acc
probit_sens = unname(probit_conftable$table[2,2]/(colSums(probit_conftable$table))[2])
probit_spec =  unname(probit_conftable$table[1,1]/(colSums(probit_conftable$table))[1])
probit_kap = unname(probit_conftable$overall['Kappa'])

probit_ho_modelfitmetrics = c(probit_acc,probit_err,probit_sens,probit_spec,probit_kap)
names(probit_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
probit_ho_modelfitmetrics

############################
###Cross Validation
require(caret)

#10-fold
set.seed(13)
folds_probit_cv10 =createFolds(stmath$paid,k=10)
probit_cv10 = sapply(folds_probit_cv10, function(x)
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_CV10=rep(0,nrow(testset))
  predclass_probit_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_cv10_modelfitmetrics = apply(probit_cv10,1,mean)
probit_cv10_modelfitmetrics

############################
###5-fold
set.seed(12)
folds_probit_cv5 =createFolds(stmath$paid,k=5)
probit_cv5 = sapply(folds_probit_cv5, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_CV5=rep(0,nrow(testset))
  predclass_probit_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_cv5_modelfitmetrics = apply(probit_cv5,1,mean)
probit_cv5_modelfitmetrics

############################
###LOOCV
set.seed(11)
folds_probit_cvlo =createFolds(stmath$paid,k=394)
probit_cvlo = sapply(folds_probit_cvlo, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_CVlo=rep(0,nrow(testset))
  predclass_probit_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_cvlo_modelfitmetrics = apply(probit_cvlo,1,mean)
probit_cvlo_modelfitmetrics


###############################
###Combination
comp_probit = rbind(probit_ho_modelfitmetrics,probit_cv10_modelfitmetrics, probit_cv5_modelfitmetrics
                    , probit_cvlo_modelfitmetrics)
comp_probit

##############################################################################################
###Logit
###HO
set.seed(10)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathlogit_train=stmath[stratid,] ## Creating the Training dataset
stmathlogit_test=stmath[-stratid,] ## Creating the test data set

###Using the logit model on training data
mod_logit_train=glm(paid~.,data=stmathlogit_train,family=binomial(link="logit"))
summary(mod_logit_train)
pred_logit=predict(mod_logit_train, newdata = stmathlogit_test,type = "response")

###Lets look at prediction quality
predclass_logit=rep(0,nrow(stmathlogit_test))

predclass_logit[pred_logit>0.5]=1

logit_conftable=confusionMatrix(as.factor(predclass_logit),as.factor(stmathlogit_test$paid), 
                                positive="1")
###Testing
logit_acc = mean(predclass_logit==stmathlogit_test$paid)
logit_err = 1-logit_acc
logit_sens = unname(logit_conftable$table[2,2]/(colSums(logit_conftable$table))[2])
logit_spec =  unname(logit_conftable$table[1,1]/(colSums(logit_conftable$table))[1])
logit_kap = unname(logit_conftable$overall['Kappa'])

logit_ho_modelfitmetrics = c(logit_acc,logit_err,logit_sens,logit_spec,logit_kap)
names(logit_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')

logit_ho_modelfitmetrics
################################
###Cross Validation
###10-fold
set.seed(9)
folds_logit_cv10 =createFolds(stmath$paid,k=10)
logit_cv10 = sapply(folds_logit_cv10, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_CV10=rep(0,nrow(testset))
  predclass_logit_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_cv10_modelfitmetrics = apply(logit_cv10,1,mean)
logit_cv10_modelfitmetrics

###########################
###5-fold
set.seed(8)
folds_logit_cv5 =createFolds(stmath$paid,k=5)

logit_cv5 = sapply(folds_logit_cv5, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_CV5=rep(0,nrow(testset))
  predclass_logit_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_cv5_modelfitmetrics = apply(logit_cv5,1,mean)
logit_cv5_modelfitmetrics

##########################
###LOOCV
set.seed(7)
folds_logit_cvlo =createFolds(stmath$paid,k=394)

logit_cvlo = sapply(folds_logit_cvlo, function(x)
  
{
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_CVlo=rep(0,nrow(testset))
  predclass_logit_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_cvlo_modelfitmetrics = apply(logit_cvlo,1,mean)
logit_cvlo_modelfitmetrics
###########################################################
###Combination 
comp_logit = rbind(logit_ho_modelfitmetrics,logit_cv10_modelfitmetrics, logit_cv5_modelfitmetrics
                   , logit_cvlo_modelfitmetrics)
comp_logit
################################################################################################
###Comparing Probit and Logit

comp_probit_logit = rbind(comp_probit, comp_logit)
comp_probit_logit
################################################################################################

###Replicating the above with edited dataset, removing the negative coefficient variables
###obtained by Random Forest

###Removing the negative coefficient variables from the stmath dataset; (nursery and traveltime...)
stmath_adj = as.data.frame(subset(na.omit(stmath), select = -c(nursery, freetime, absences, Fedu, 
                                                               goout,school, guardian,romantic,internet, address,traveltime)))
###Checking again
str(stmath_adj)
dim(stmath_adj)
attach(stmath_adj)

set.seed(6)

###Probit_adjusted dataset
###HO
stratid_adj=createDataPartition(stmath_adj$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmath_adj_probit_train=stmath_adj[stratid_adj,] ## Creating the Training dataset
stmath_adj_probit_test=stmath_adj[-stratid_adj,] ## Creating the test data set

#Using the probit model on training data
mod_adj_probit_train=glm(paid~.,data=stmath_adj_probit_train,family=binomial(link="probit"))
summary(mod_adj_probit_train)
pred_adj_probit=predict(mod_adj_probit_train, newdata = stmath_adj_probit_test,type = "response")

###Lets look at prediction quality
predclass_adj_probit=rep(0,nrow(stmath_adj_probit_test))
predclass_adj_probit[pred_adj_probit>0.5]=1
probit_adj_conftable=confusionMatrix(as.factor(predclass_adj_probit),as.factor(stmath_adj_probit_test$paid), 
                                     positive="1")

###Testing
probit_adj_acc = mean(predclass_adj_probit==stmath_adj_probit_test$paid)
probit_adj_err = 1-probit_adj_acc
probit_adj_sens = unname(probit_adj_conftable$table[2,2]/(colSums(probit_adj_conftable$table))[2])
probit_adj_spec =  unname(probit_adj_conftable$table[1,1]/(colSums(probit_adj_conftable$table))[1])
probit_adj_kap = unname(probit_adj_conftable$overall['Kappa'])

probit_adj_ho_modelfitmetrics = c(probit_adj_acc,probit_adj_err,probit_adj_sens,probit_adj_spec,probit_adj_kap)
names(probit_adj_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
probit_adj_ho_modelfitmetrics
############################
#10-fold
set.seed(5)
folds_probit_adj_cv10 =createFolds(stmath_adj$paid,k=10)
probit_adj_cv10 = sapply(folds_probit_adj_cv10, function(x)
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_adj_probit_CV10=rep(0,nrow(testset))
  predclass_adj_probit_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_adj_probit_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_adj_cv10_modelfitmetrics = apply(probit_adj_cv10,1,mean)
probit_adj_cv10_modelfitmetrics
############################
###5-fold
set.seed(4)
folds_probit_adj_cv5 =createFolds(stmath_adj$paid,k=5)
probit_adj_cv5 = sapply(folds_probit_adj_cv5, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_adj_CV5=rep(0,nrow(testset))
  predclass_probit_adj_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_adj_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_adj_cv5_modelfitmetrics = apply(probit_adj_cv5,1,mean)
probit_adj_cv5_modelfitmetrics
############################
###LOOCV
set.seed(3)
folds_probit_adj_cvlo =createFolds(stmath_adj$paid,k=394)
probit_adj_cvlo = sapply(folds_probit_adj_cvlo, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="probit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_probit_adj_CVlo=rep(0,nrow(testset))
  predclass_probit_adj_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_probit_adj_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

probit_adj_cvlo_modelfitmetrics = apply(probit_adj_cvlo,1,mean)
probit_adj_cvlo_modelfitmetrics
###############################################################################################
###Computation combination for probit adjusted dataset
comp_probit_adj = rbind(probit_adj_ho_modelfitmetrics,probit_adj_cv10_modelfitmetrics, probit_adj_cv5_modelfitmetrics
                        , probit_adj_cvlo_modelfitmetrics)

comp_probit_adj
###Logit_adjusted dataset
###HO
set.seed(30)
stratid_logit_adj=createDataPartition(stmath_adj$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmath_logit_adj_train=stmath_adj[stratid_logit_adj,] ## Creating the Training dataset
stmath_logit_adj_test=stmath_adj[-stratid_logit_adj,] ## Creating the test data set

###Using the logit model on training data
mod_logit_adj_train=glm(paid~.,data=stmath_logit_adj_train,family=binomial(link="logit"))
summary(mod_logit_adj_train)
pred_logit_adj=predict(mod_logit_adj_train, newdata = stmath_logit_adj_test,type = "response")

###Lets look at prediction quality
predclass_logit_adj=rep(0,nrow(stmath_logit_adj_test))

predclass_logit_adj[pred_logit_adj>0.5]=1

logit_adj_conftable=confusionMatrix(as.factor(predclass_logit_adj),as.factor(stmath_logit_adj_test$paid), 
                                    positive="1")
###Testing
logit_adj_acc = mean(predclass_logit_adj==stmath_logit_adj_test$paid)
logit_adj_err = 1-logit_adj_acc
logit_adj_sens = unname(logit_adj_conftable$table[2,2]/(colSums(logit_adj_conftable$table))[2])
logit_adj_spec =  unname(logit_adj_conftable$table[1,1]/(colSums(logit_adj_conftable$table))[1])
logit_adj_kap = unname(logit_adj_conftable$overall['Kappa'])

logit_adj_ho_modelfitmetrics = c(logit_adj_acc,logit_adj_err,logit_adj_sens,logit_adj_spec,logit_adj_kap)
names(logit_adj_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')

logit_adj_ho_modelfitmetrics
################################
###Cross Validation
###10-fold
set.seed(29)
folds_logit_adj_cv10 =createFolds(stmath_adj$paid,k=10)
logit_adj_cv10 = sapply(folds_logit_adj_cv10, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_adj_CV10=rep(0,nrow(testset))
  predclass_logit_adj_CV10[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_adj_CV10),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_adj_cv10_modelfitmetrics = apply(logit_adj_cv10,1,mean)
logit_adj_cv10_modelfitmetrics

###########################
###5-fold
set.seed(28)
folds_logit_adj_cv5 =createFolds(stmath_adj$paid,k=5)

logit_adj_cv5 = sapply(folds_logit_adj_cv5, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_adj_CV5=rep(0,nrow(testset))
  predclass_logit_adj_CV5[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor(predclass_logit_adj_CV5),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_adj_cv5_modelfitmetrics = apply(logit_adj_cv5,1,mean)
logit_adj_cv5_modelfitmetrics
##########################
###LOOCV
set.seed(27)
folds_logit_adj_cvlo =createFolds(stmath_adj$paid,k=394)

logit_adj_cvlo = sapply(folds_logit_adj_cvlo, function(x)
  
{
  trainset=stmath_adj[-x,]
  testset=stmath_adj[x,]
  
  
  model=glm(paid~.,data=trainset,
            family=binomial(link="logit"))
  
  ppred=predict(model, newdata = testset,type = "response")
  
  predclass_logit_adj_CVlo=rep(0,nrow(testset))
  predclass_logit_adj_CVlo[ppred>0.5]=1
  
  obj=confusionMatrix(as.factor( predclass_logit_adj_CVlo),as.factor(testset$paid), 
                      positive="1")
  
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate','Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

logit_adj_cvlo_modelfitmetrics = apply(logit_adj_cvlo,1,mean)
logit_adj_cvlo_modelfitmetrics
###############################################################################################
###Computation Combination for the logit adjusted
comp_logit_adj = rbind(logit_adj_ho_modelfitmetrics,logit_adj_cv10_modelfitmetrics, logit_adj_cv5_modelfitmetrics
                       , logit_adj_cvlo_modelfitmetrics)
comp_logit_adj

###############################################################################################
###Comparing Probit a
nd Logit adjusted dataset

comp_probit_logit_adj = rbind(comp_probit_adj, comp_logit_adj)
comp_probit_logit_adj
###############################################################################################
###Comparing Probit and Logit adjusted and normal dataset
comp_probit_logit_both = rbind(comp_probit_logit, comp_probit_logit_adj)
comp_probit_logit_both
###############################################################################################

#### LDA, QDA and SVM
getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')
stmath = as.data.frame(subset(na.omit(stmath)))
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))
dim(stmath)
names(stmath)
str(stmath)
attach(stmath)

stmath$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath$paid = as.factor(ifelse(paid=='no', 0,1))
stmath$activities = as.factor(ifelse(activities=='no', 0,1))
stmath$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath$higher = as.factor(ifelse(higher=='no', 0,1))
stmath$internet = as.factor(ifelse(internet=='no', 0,1))
stmath$romantic = as.factor(ifelse(romantic=='no', 0,1))

stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

plot(stmath$paid)

###LDA + QDA

#Preparing the data
attach(stmath)
stmath$schoolsup = as.factor(stmath$schoolsup)
stmath$sex = as.factor(stmath$sex)
stmath$guardian = as.factor(stmath$guardian)
stmath$famsup = as.factor(stmath$famsup)
stmath$paid = as.factor(stmath$paid)
stmath$activities = as.factor(stmath$activities)
stmath$nursery = as.factor(stmath$nursery)
stmath$higher = as.factor(stmath$higher)
stmath$internet = as.factor(stmath$internet)
stmath$romantic = as.factor(stmath$romantic)
stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

#LDA
#Holdout Sampling - LDA
library(MASS)

#Creating the training and test data set
library(caret) 
set.seed(06)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathlda_train=stmath[stratid,] ## Creating the Training dataset
stmathlda_test=stmath[-stratid,] ## Creating the test data set

mod_lda = lda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc+higher+activities, data = stmathlda_train)
mod_lda 

pred_lda_ho = predict(mod_lda, newdata = stmathlda_test)
head(pred_lda_ho$posterior)
head(pred_lda_ho$class)

lda_ho_conftable = table(pred_lda_ho$class, stmathlda_test$paid)
lda_ho_conftable2 = confusionMatrix(as.factor(pred_lda_ho$class),as.factor(stmathlda_test$paid), 
                                    positive="1")
lda_ho_conftable2

lda_ho_acc = mean(pred_lda_ho$class==stmathlda_test$paid)
lda_ho_err = 1-lda_ho_acc
lda_ho_sens = unname(lda_ho_conftable2$table[2,2]/(colSums(lda_ho_conftable2$table))[2])
lda_ho_spec =  unname(lda_ho_conftable2$table[1,1]/(colSums(lda_ho_conftable2$table))[1])
lda_ho_kap = unname(lda_ho_conftable2$overall['Kappa'])

lda_ho_modelfitmetrics = c(lda_ho_acc,lda_ho_err,lda_ho_sens,lda_ho_spec,lda_ho_kap)
names(lda_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
lda_ho_modelfitmetrics

#CV - LDA
require(caret)
#10-fold
set.seed(07)
folds_lda_cv10 =createFolds(stmath$paid,k=10)

lda_cv10 = sapply(folds_lda_cv10, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=lda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc+higher+activities, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1],(1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'], unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

lda_cv10_modelfitmetrics = apply(lda_cv10,1,mean)
lda_cv10_modelfitmetrics

#5-fold
set.seed(08)
folds_lda_cv5 =createFolds(stmath$paid,k=5)

lda_cv5 = sapply(folds_lda_cv5, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=lda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc+higher+activities, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

lda_cv5_modelfitmetrics = apply(lda_cv5,1,mean)
lda_cv5_modelfitmetrics

#LOOCV
set.seed(09)
folds_lda_cvlo =createFolds(stmath$paid,k=394)

lda_cvlo = sapply(folds_lda_cvlo, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=lda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc+higher+activities, data=trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

lda_cvlo_modelfitmetrics = apply(lda_cvlo,1,mean)
lda_cvlo_modelfitmetrics

comp_lda = rbind(lda_ho_modelfitmetrics,lda_cv10_modelfitmetrics, lda_cv5_modelfitmetrics, lda_cvlo_modelfitmetrics)
comp_lda

#QDA
#Holdout Sampling - QDA
#Creating the training and test data set
set.seed(10)
stratid=createDataPartition(stmath$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
stmathqda_train=stmath[stratid,] ## Creating the Training dataset
stmathqda_test=stmath[-stratid,] ## Creating the test data set

mod_qda = qda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc,data = stmathqda_train)
mod_qda 

pred_qda_ho = predict(mod_qda, newdata = stmathqda_test)
head(pred_qda_ho$posterior)
head(pred_qda_ho$class)

qda_ho_conftable = table(pred_qda_ho$class, stmathqda_test$paid)
qda_ho_conftable2 = confusionMatrix(as.factor(pred_qda_ho$class),as.factor(stmathqda_test$paid), 
                                    positive="1")
qda_ho_conftable2

qda_ho_acc = mean(pred_qda_ho$class==stmathqda_test$paid)
qda_ho_err = 1-qda_ho_acc
qda_ho_sens = unname(qda_ho_conftable2$table[2,2]/(colSums(qda_ho_conftable2$table))[2])
qda_ho_spec =  unname(qda_ho_conftable2$table[1,1]/(colSums(qda_ho_conftable2$table))[1])
qda_ho_kap = unname(qda_ho_conftable2$overall['Kappa'])

qda_ho_modelfitmetrics = c(qda_ho_acc,qda_ho_err,qda_ho_sens,qda_ho_spec,qda_ho_kap)
names(qda_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
qda_ho_modelfitmetrics

#CV - QDA
#10-fold
set.seed(11)
folds_qda_cv10 =createFolds(stmath$paid,k=10)

qda_cv10 = sapply(folds_qda_cv10, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=qda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

qda_cv10_modelfitmetrics = apply(qda_cv10,1,mean)
qda_cv10_modelfitmetrics

#5-fold
set.seed(12)
folds_qda_cv5 =createFolds(stmath$paid,k=5)

qda_cv5 = sapply(folds_qda_cv5, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  
  model=qda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc, data = trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

qda_cv5_modelfitmetrics = apply(qda_cv5,1,mean)
qda_cv5_modelfitmetrics

#LOOCV
set.seed(13)
folds_qda_cvlo =createFolds(stmath$paid,k=394)

qda_cvlo = sapply(folds_qda_cvlo, function(x){
  trainset=stmath[-x,]
  testset=stmath[x,]
  model=qda(paid~famsup+reason+Mjob+G3+Fjob+Medu+Dalc+Pstatus+studytime+age+failures+schoolsup+health+famsize+famrel+sex+Walc, data=trainset)
  ppred=predict(model, testset)
  obj=confusionMatrix(as.factor(ppred$class),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

qda_cvlo_modelfitmetrics = apply(qda_cvlo,1,mean)
qda_cvlo_modelfitmetrics

comp_qda = rbind(qda_ho_modelfitmetrics,qda_cv10_modelfitmetrics, qda_cv5_modelfitmetrics, qda_cvlo_modelfitmetrics)
comp_qda

#Comparing LDA and QDA

comp_ldaqda = rbind(comp_lda, comp_qda)
comp_ldaqda

#SVM
#1-Normalizing our data
normalize = function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

non_num_var = stmath[,c(1,2,4,5,6,9,10,11,12,16,17,18,19,20,21,22,23)]
num_var = stmath[, c(3,7,8, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31) ]

Pnorm = as.data.frame(lapply(num_var, normalize))
Pnormed=cbind(Pnorm, non_num_var)
names(Pnormed)

#Creating the Testing and Training datasets using hold out sampling
set.seed(14)
stratid=createDataPartition(Pnormed$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
Normstmath_train=Pnormed[stratid,] ## Creating the Training dataset
Normstmath_test=Pnormed[-stratid,] ## Creating the test data set

#Run the SVM model
#Linear Kernel
mod_svm_lin = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="linear", cost=20, scale=FALSE)
summary(mod_svm_lin)

svm_pred_lin = predict(mod_svm_lin, newdata=Normstmath_test)
svm_lin_conftable = confusionMatrix(as.factor(svm_pred_lin),as.factor(Normstmath_test$paid), 
                                    positive="1")

svmlin_ho_acc = unname(svm_lin_conftable$overall[1])
svmlin_ho_err = 1-svmlin_ho_acc
svmlin_ho_sens = unname(svm_lin_conftable$table[2,2]/(colSums(svm_lin_conftable$table))[2])
svmlin_ho_spec =  unname(svm_lin_conftable$table[1,1]/(colSums(svm_lin_conftable$table))[1])
svmlin_ho_kap = unname(svm_lin_conftable$overall['Kappa'])

svmlin_ho_modelfitmetrics = c(svmlin_ho_acc,svmlin_ho_err,svmlin_ho_sens,svmlin_ho_spec,svmlin_ho_kap)
names(svmlin_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmlin_ho_modelfitmetrics

#Polynomial Kernel
mod_svm_poly = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="polynomial", cost=20, scale=FALSE)
summary(mod_svm_poly)

svm_pred_poly = predict(mod_svm_poly, newdata=Normstmath_test)
svm_pol_conftable = confusionMatrix(as.factor(svm_pred_poly),as.factor(Normstmath_test$paid), 
                                    positive="1")

svmpol_ho_acc = unname(svm_pol_conftable$overall[1])
svmpol_ho_err = 1-svmpol_ho_acc
svmpol_ho_sens = unname(svm_pol_conftable$table[2,2]/(colSums(svm_pol_conftable$table))[2])
svmpol_ho_spec =  unname(svm_pol_conftable$table[1,1]/(colSums(svm_pol_conftable$table))[1])
svmpol_ho_kap = unname(svm_pol_conftable$overall['Kappa'])

svmpol_ho_modelfitmetrics = c(svmpol_ho_acc,svmpol_ho_err,svmpol_ho_sens,svmpol_ho_spec,svmpol_ho_kap)
names(svmpol_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmpol_ho_modelfitmetrics

#Radial Kernel
mod_svm_rad = svm(as.factor(Normstmath_train$paid)~., data=Normstmath_train, kernel="radial", cost=20, scale=FALSE)
summary(mod_svm_rad)

svm_pred_rad =predict(mod_svm_rad, newdata=Normstmath_test)
svm_rad_conftable = confusionMatrix(as.factor(svm_pred_rad),as.factor(Normstmath_test$paid), 
                                    positive="1")

svmrad_ho_acc = unname(svm_rad_conftable$overall[1])
svmrad_ho_err = 1-svmrad_ho_acc
svmrad_ho_sens = unname(svm_rad_conftable$table[2,2]/(colSums(svm_rad_conftable$table))[2])
svmrad_ho_spec =  unname(svm_rad_conftable$table[1,1]/(colSums(svm_rad_conftable$table))[1])
svmrad_ho_kap = unname(svm_rad_conftable$overall['Kappa'])

svmrad_ho_modelfitmetrics = c(svmrad_ho_acc,svmrad_ho_err,svmrad_ho_sens,svmrad_ho_spec,svmrad_ho_kap)
names(svmrad_ho_modelfitmetrics) = c('Accuracy', 'Error rate', 'Sensitivity', 'Specificity', 'Kappa')
svmrad_ho_modelfitmetrics

#Comparing linear, polynomial and radial kernels
comp_svm_ho = rbind(svmlin_ho_modelfitmetrics,svmpol_ho_modelfitmetrics,svmrad_ho_modelfitmetrics) 
comp_svm_ho
#Polynomial seems to be most precise

##Cross validation of the SVM with the data
#10-fold
svm_folds_cv10 = createFolds(Pnormed$paid,k=10)

svm_cv10 = sapply(svm_folds_cv10, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

svm_cv10_modelfitmetrics = apply(svm_cv10,1,mean)
svm_cv10_modelfitmetrics

#5-fold
svm_folds_cv5 = createFolds(Pnormed$paid,k=5)

svm_cv5 = sapply(svm_folds_cv5, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

svm_cv5_modelfitmetrics = apply(svm_cv5,1,mean)
svm_cv5_modelfitmetrics

#LOOCV
svm_folds_cvlo = createFolds(Pnormed$paid,k= 394)

svm_cvlo = sapply(svm_folds_cvlo, function(x){
  trainset=Pnormed[-x,]
  testset=Pnormed[x,]
  
  model=svm(as.factor(trainset$paid)~., data=trainset, kernel="polynomial", cost=20, scale=FALSE)
  ppred=predict(model, newdata=testset)
  obj=confusionMatrix(as.factor(ppred),as.factor(testset$paid), 
                      positive="1")
  metrics=c(obj$overall[1], (1-obj$overall[1]), obj$byClass['Sensitivity'],obj$byClass['Specificity'] , unname(obj$overall['Kappa']))
  names(metrics) = c('Accuracy', 'Error rate', 'Sensitivity','Specificity', 'Kappa')
  return(metrics)
})

svm_cvlo_modelfitmetrics = apply(svm_cvlo,1,mean)
svm_cvlo_modelfitmetrics

#comparing svm across CV ks:
comp_svms = rbind(comp_svm_ho, svm_cv10_modelfitmetrics,svm_cv5_modelfitmetrics,svm_cvlo_modelfitmetrics)
comp_svms
comp_ldaqda


#### kNN and Naive Bayes

#0 - Data Manipulation

getwd()
setwd('~/Desktop')

stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)

stmath_kNN_NB = stmath
stmath_kNN_NB = as.data.frame(subset(na.omit(stmath_kNN_NB)))
stmath_kNN_NB = stmath_kNN_NB[, -c(9, 10, 11, 31, 32)]
str(stmath_kNN_NB)
attach(stmath_kNN_NB)

stmath_kNN_NB$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath_kNN_NB$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath_kNN_NB$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath_kNN_NB$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath_kNN_NB$paid = as.factor(ifelse(paid=='no', 0,1))
stmath_kNN_NB$activities = as.factor(ifelse(activities=='no', 0,1))
stmath_kNN_NB$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath_kNN_NB$higher = as.factor(ifelse(higher=='no', 0,1))
stmath_kNN_NB$internet = as.factor(ifelse(internet=='no', 0,1))
stmath_kNN_NB$romantic = as.factor(ifelse(romantic=='no', 0,1))

stmath_kNN_NB$school = as.factor(ifelse(school=='GP', 0,1))
stmath_kNN_NB$address = as.factor(ifelse(address=='R', 0,1))
stmath_kNN_NB$famsize = as.factor(ifelse(famsize=='LE3', 0,1))
stmath_kNN_NB$Pstatus = as.factor(ifelse(Pstatus=='A', 0,1))

## kNN

## Normalizing our data
normalize_kNN_NB = function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

non_num_var_kNN_NB = stmath_kNN_NB[, c(1,2,4,5,6,9,13,14,15,16,17,18,19,20)]
num_var_kNN_NB = stmath_kNN_NB[, c(3,7,8,10,11,12,21,22,23,24,25,26,27,28)]

Pnorm_kNN_NB = as.data.frame(lapply(num_var_kNN_NB, normalize_kNN_NB))
Pnormed_kNN_NB=cbind(Pnorm_kNN_NB, non_num_var_kNN_NB)
names(Pnormed_kNN_NB)

# Creating testing and training data sets

library(caret) 
set.seed(14)
stratid_kNN_NB=createDataPartition(Pnormed_kNN_NB$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
Normstmath_kNN_NB_train=Pnormed_kNN_NB[stratid_kNN_NB,] ## Creating the Training dataset
Normstmath_kNN_NB_test=Pnormed_kNN_NB[-stratid_kNN_NB,] ## Creating the test data set

## Run the KNN algorithm on the Training data set

library(class)

knn_mod = knn(train=Normstmath_kNN_NB_train[,-23], test=Normstmath_kNN_NB_test[,-23], cl=Normstmath_kNN_NB_train[,23], k=16)

## Test model predictions using the confusion matrix

library(e1071)
confusionMatrix(as.factor(knn_mod),as.factor(Normstmath_kNN_NB_test[,23]), 
                positive="1")

## KNN with cross validation
folds_kNN_NB=createFolds(Pnormed_kNN_NB$paid,k=10)

knn_cvaccuracy = sapply(folds_kNN_NB, function(x){
  trainset=Pnormed_kNN_NB[-x,]
  testset=Pnormed_kNN_NB[x,]
  model=knn(trainset[,-23], test=testset[,-23], cl=trainset[,23], k=16)
  obj=confusionMatrix(as.factor(model),as.factor(testset[,23]), 
                      positive="1")
  metrics_kNN_NB=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics_kNN_NB)
})

knn_cvaccuracy

### Naive Bayes Classifier

library(e1071)

## Holdout sampling apporach

Normstmath_kNN_NB_train_nb=Normstmath_kNN_NB_train[,-23] ## Pulling the features out from the train set
Normstmath_kNN_NB_test_nb=Normstmath_kNN_NB_test[,-23] ## Pulling the features set out of the test set

nbmath=naiveBayes(Normstmath_kNN_NB_train_nb, as.factor(Normstmath_kNN_NB_train[,23])) ## Run the Naive Bayes model

nbpred=predict(nbmath, Normstmath_kNN_NB_test_nb, type="class") ## Generate Test Predictions


## Generate the confusion matrix

confusionMatrix(as.factor(nbpred),as.factor(Normstmath_kNN_NB_test[,23]), 
                positive="1")

## Naive Bayes with cross validation
nb_folds=createFolds(Pnormed_kNN_NB$paid,k=10)

nb_cvaccuracy = sapply(folds_kNN_NB, function(x){
  trainset=Pnormed_kNN_NB[-x,]
  testset=Pnormed_kNN_NB[x,]
  
  trainx=trainset[,-23]
  testx=testset[,-23]
  
  model=naiveBayes(trainx, as.factor(trainset[,23]))
  npred=predict(model, testx, type="class")
  
  obj=confusionMatrix(as.factor(npred),as.factor(testset[,23]), 
                      positive="1")
  metrics=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics)
})

nb_cvaccuracy

## Running kNN and Naive Bayes with variables removed

remove(stmath_kNN_NB)

stmath_kNN_NB = stmath

dim(stmath_kNN_NB)
names(stmath_kNN_NB)

#0 - Data Manipulation
stmath_kNN_NB = as.data.frame(subset(na.omit(stmath_kNN_NB)))
stmath_kNN_NB = stmath_kNN_NB[, -c(1, 8, 9, 10, 11, 12, 13, 20, 22, 23, 25, 26, 30, 31, 32)]
str(stmath_kNN_NB)
attach(stmath_kNN_NB)

stmath_kNN_NB$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath_kNN_NB$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath_kNN_NB$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath_kNN_NB$paid = as.factor(ifelse(paid=='no', 0,1))
stmath_kNN_NB$activities = as.factor(ifelse(activities=='no', 0,1))
stmath_kNN_NB$higher = as.factor(ifelse(higher=='no', 0,1))

stmath_kNN_NB$address = as.factor(ifelse(address=='R', 0,1))
stmath_kNN_NB$famsize = as.factor(ifelse(famsize=='LE3', 0,1))
stmath_kNN_NB$Pstatus = as.factor(ifelse(Pstatus=='A', 0,1))

## kNN

LESSnon_num_var = stmath_kNN_NB[, c(1,3,4,5,9,10,11,12,13)]
LESSnum_var = stmath_kNN_NB[, c(2,6,7,8,14,15,16,17,18)]

LESSPnorm = as.data.frame(lapply(LESSnum_var, normalize_kNN_NB))
LESSPnormed=cbind(LESSPnorm, LESSnon_num_var)
names(LESSPnormed)

# Creating testing and training data sets

library(caret) 
set.seed(14)
LESSstratid=createDataPartition(LESSPnormed$paid, p=0.75, list=FALSE) ## Create the partition again setting 75% to training
LESSNormstmath_train=LESSPnormed[LESSstratid,] ## Creating the Training dataset
LESSNormstmath_test=LESSPnormed[-LESSstratid,] ## Creating the test data set

## Run the KNN algorithm on the Training data set

library(class)

LESSknn_mod = knn(train=LESSNormstmath_train[,-16], test=LESSNormstmath_test[,-16], cl=LESSNormstmath_train[,16], k=16)

## Test model predictions using the confusion matrix

library(e1071)
confusionMatrix(as.factor(LESSknn_mod),as.factor(LESSNormstmath_test[,16]), 
                positive="1")

## KNN with cross validation
LESSfolds=createFolds(LESSPnormed$paid,k=10)

LESSknn_cvaccuracy = sapply(LESSfolds, function(x){
  trainset=LESSPnormed[-x,]
  testset=LESSPnormed[x,]
  model=knn(trainset[,-16], test=testset[,-16], cl=trainset[,16], k=16)
  obj=confusionMatrix(as.factor(model),as.factor(testset[,16]), 
                      positive="1")
  metrics=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics)
})

LESSknn_cvaccuracy

### Naive Bayes Classifier

library(e1071)

## Holdout sampling apporach

LESSNormstmath_train_nb=LESSNormstmath_train[,-16] ## Pulling the features out from the train set
LESSNormstmath_test_nb=LESSNormstmath_test[,-16] ## Pulling the features set out of the test set

LESSnbmath=naiveBayes(LESSNormstmath_train_nb, as.factor(LESSNormstmath_train[,16])) ## Run the Naive Bayes model

LESSnbpred=predict(LESSnbmath, LESSNormstmath_test_nb, type="class") ## Generate Test Predictions


## Generate the confusion matrix

confusionMatrix(as.factor(LESSnbpred),as.factor(LESSNormstmath_test[,16]), 
                positive="1")

## Naive Bayes with cross validation
LESSnb_folds=createFolds(LESSPnormed$paid,k=10)

LESSnb_cvaccuracy = sapply(LESSfolds, function(x){
  trainset=LESSPnormed[-x,]
  testset=LESSPnormed[x,]
  
  LESStrainx=trainset[,-16]
  LESStestx=testset[,-16]
  
  model=naiveBayes(LESStrainx, as.factor(trainset[,16]))
  npred=predict(model, LESStestx, type="class")
  
  obj=confusionMatrix(as.factor(npred),as.factor(testset[,16]), 
                      positive="1")
  metrics=c(obj$overall[1], obj$overall[2], obj$byClass[1], obj$byClass[2])
  return(metrics)
})

LESSnb_cvaccuracy

## End of kNN and Naive Bayes section.

#### Additional analysis for conclusion
#Additional Computations
getwd()
setwd('C:/Users/stefa/OneDrive/Desktop/Docs/02_HEC/13_Second Semester/02_Data and Decision_Applied Analytics/04_Group project')
stmath = read.csv('student-mat.csv', header = T, sep = ',')

dim(stmath)
names(stmath)
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))
str(stmath)
attach(stmath)


#0 - Data Manipulation
stmath = as.data.frame(subset(na.omit(stmath), select = -c(G1, G2)))
str(stmath)
attach(stmath)

stmath$schoolsup = as.factor(ifelse(schoolsup=='no', 0,1))
stmath$sex = as.factor(ifelse(sex== 'M', 0,1))
stmath$guardian = as.factor(ifelse(guardian == 'father', 0,1))
stmath$famsup = as.factor(ifelse(famsup=='no', 0,1))
stmath$paid = as.factor(ifelse(paid=='no', 0,1))
stmath$activities = as.factor(ifelse(activities=='no', 0,1))
stmath$nursery = as.factor(ifelse(nursery=='no', 0,1))
stmath$higher = as.factor(ifelse(higher=='no', 0,1))
stmath$internet = as.factor(ifelse(internet=='no', 0,1))
stmath$romantic = as.factor(ifelse(romantic=='no', 0,1))

stmath$school = as.factor(stmath$school)
stmath$address = as.factor(stmath$address)
stmath$famsize = as.factor(stmath$famsize)
stmath$Pstatus = as.factor(stmath$Pstatus)
stmath$Mjob = as.factor(stmath$Mjob)
stmath$Fjob = as.factor(stmath$Fjob)
stmath$reason = as.factor(stmath$reason)

stmath$schoolsup = as.numeric(stmath$schoolsup)
stmath$sex = as.numeric(stmath$sex)
stmath$guardian = as.numeric(stmath$guardian)
stmath$famsup = as.numeric(stmath$famsup)
stmath$paid = as.numeric(stmath$paid)
stmath$activities = as.numeric(stmath$activities)
stmath$nursery = as.numeric(stmath$nursery)
stmath$higher = as.numeric(stmath$higher)
stmath$internet = as.numeric(stmath$internet)
stmath$romantic = as.numeric(stmath$romantic)

#Bringing binary variable back to 1/0 instead of 2/1
attach(stmath)
stmath$schoolsup = ifelse(schoolsup==1, 0,1)
stmath$sex = ifelse(sex== 1, 0,1)
stmath$guardian = ifelse(guardian == 1, 0,1)
stmath$famsup = ifelse(famsup==1, 0,1)
stmath$paid = ifelse(paid==1, 0,1)
stmath$activities = ifelse(activities==1, 0,1)
stmath$nursery = ifelse(nursery==1, 0,1)
stmath$higher = ifelse(higher==1, 0,1)
stmath$internet = ifelse(internet==1, 0,1)
stmath$romantic = ifelse(romantic==1, 0,1)


#interesting statistics
#famsup
attach(stmath)
mod_famsup = lm(stmath$paid~famsup+reason+famsize + G3 + famsup:G3 + famsup:reason, data = stmath)
summary(mod_famsup)

#Mjob + Fjob
stmath$Fteach = as.factor(ifelse(stmath$Fjob == 'teacher', 1, 0))
stmath$Mteach = as.factor(ifelse(stmath$Mjob == 'teacher', 1, 0))
mod_mfteach = lm(paid~famsup+reason+famsize + G3 + famsup:G3 + famsup:reason + Mteach + Fteach + Mteach:Fteach, data = stmath)
summary(mod_mfteach)

#G3
#New insight
mod_g3 = lm(G3~., data = stmath)
summary(mod_g3)

#Old code - souce of error! Difference is actually not statistically significant
subset_score_paid = subset(stmath, stmath$paid == '1')
mean_score_paid = mean(subset_score_paid[,'G3'])
mean_score_paid
subset_score_nopaid = subset(stmath, stmath$paid == '0')
mean_score_nopaid = mean(subset_score_nopaid[,'G3'])
mean_score_nopaid

#Reason
stmath$coavail = as.factor(ifelse(stmath$reason == 'course', 1, 0))
mod_coavail = lm(G3~coavail, data = stmath)
summary(mod_coavail)
